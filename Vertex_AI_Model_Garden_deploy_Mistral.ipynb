{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Mistral Model (Deployment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates deploying prebuilt Mistral models in Vertex AI.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy prebuilt [Mistral models](https://huggingface.co/mistralai) with [vLLM](https://github.com/vllm-project/vllm) containers\n",
        "    - [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1): pretrained generative text model with 7 billion parameters\n",
        "    - [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1): Instruction fine-tuned version of the Mistral-7B-v0.1 generative text model\n",
        "    - [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2): Improved instruction fine-tuned version of Mistral-7B-Instruct-v0.1 supporting 32k context length\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi-aGs442yve"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install google-cloud-aiplatform --upgrade --user --quiet\n",
        "! pip install transformers==4.34.0 --quiet\n",
        "! pip install accelerate==0.23.0 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_pYs9-fyxN0",
        "outputId": "b0ac5465-2979-4890-e906-7b4c8a227746"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YwKW5MQYzQDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "GNYcBb6O2h1s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Google Cloud project\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJSMcDLwqrYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead."
      ],
      "metadata": {
        "id": "YftbQexBqzO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the default cloud project id.\n",
        "PROJECT_ID = \"<Enter your project id>\"\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = \"<Enter REGION>\"\n"
      ],
      "metadata": {
        "id": "8GCxmnlnGEDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary packages.\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTmcKN6iqoYz",
        "outputId": "fd94a271-e023-4cc6-a212-2a0b6e06c0c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling Vertex AI API and Compute Engine API.\n",
            "Updated property [core/project].\n",
            "Operation \"operations/acat.p2-388046178334-c4a72a63-d289-4b17-921b-02be80848e92\" finished successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type: \"string\"}\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "eOQu4qkhrFzr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
        "\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Create a unique GCS bucket for this notebook, if not specified by the user.\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    # Create a unique GCS bucket for this notebook, if not specified by the user\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region '%s' is different from notebook region '%s'\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUPLBqOmrAQ8",
        "outputId": "85e9fbc3-002d-4870-f8d2-1878b097a529"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using this default Service Account: 388046178334-compute@developer.gserviceaccount.com\n",
            "Creating gs://suzuki-talk-to-manual-tmp-20240506170451/...\n",
            "Using this GCS Bucket: gs://suzuki-talk-to-manual-tmp-20240506170451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWS1McjDrPRM",
        "outputId": "5bc05f79-216d-4a48-fc53-00b6efbde9ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vertex AI API.\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "# The pre-built serving docker images with vLLM\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240313_0916_RC00\"\n"
      ],
      "metadata": {
        "id": "8Q3H43h9rRk7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h7-wMw81Ty4Z"
      },
      "outputs": [],
      "source": [
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    max_model_len: int = 4096,\n",
        "    gpu_memory_utilization=0.9,\n",
        "    use_openai_server: bool = False,\n",
        "    use_chat_completions_if_openai_server: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys Mistral models with vLLM on Vertex AI.\n",
        "\n",
        "    Args:\n",
        "        model_name: Display name of the model.\n",
        "        model_id: Model ID or path to model weights.\n",
        "        service_account: Service account for model uploading and deployment.\n",
        "        machine_type: Deployment machine type.\n",
        "        accelerator_type: Deployment accelerator type.\n",
        "        accelerator_count: Number of accelerators to use.\n",
        "        max_model_len: Maximum model length.\n",
        "        gpu_memory_utilization: Fraction of GPU memory to be used for the model\n",
        "            executor.\n",
        "\n",
        "    Returns:\n",
        "        Model instance and endpoint instance.\n",
        "    \"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    dtype = \"bfloat16\"\n",
        "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
        "        dtype = \"float16\"\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model=gs://vertex-model-garden-public-us/{model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "    }\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n"
        
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3PONphEqp2rE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123cbcf3-d1da-4403-afe8-5ddeac07e2a3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
            "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/388046178334/locations/us-central1/endpoints/7018831431954595840/operations/8869467315779928064\n",
            "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/388046178334/locations/us-central1/endpoints/7018831431954595840\n",
            "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
            "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/388046178334/locations/us-central1/endpoints/7018831431954595840')\n",
            "INFO:google.cloud.aiplatform.models:Creating Model\n",
            "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/388046178334/locations/us-central1/models/1080671496034058240/operations/109966040544313344\n",
            "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/388046178334/locations/us-central1/models/1080671496034058240@1\n",
            "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
            "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/388046178334/locations/us-central1/models/1080671496034058240@1')\n",
            "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/388046178334/locations/us-central1/endpoints/7018831431954595840\n",
            "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/388046178334/locations/us-central1/endpoints/7018831431954595840/operations/9136305593701629952\n",
            "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/388046178334/locations/us-central1/endpoints/7018831431954595840\n"
          ]
        }
      ],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section deploys the prebuilt Mistral model with [vLLM](https://github.com/vllm-project/vllm) on a Vertex endpoint. It takes 15 minutes to 1 hour to finish depending on the model and the accelerator.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "prebuilt_model_id = \"mistralai/Mistral-7B-v0.1\"  # @param [\"mistralai/Mistral-7B-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.2\"]\n",
        "# Find Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_TESLA_V100\", \"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_A100\"]\n",
        "\n",
        "if accelerator_type == \"NVIDIA_L4\":\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_count = 1\n",
        "elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
        "    machine_type = \"n1-standard-16\"\n",
        "    accelerator_count = 2\n",
        "elif accelerator_type == \"NVIDIA_TESLA_T4\":\n",
        "    machine_type = \"n1-standard-16\"\n",
        "    accelerator_count = 2\n",
        "elif accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "    machine_type = \"a2-highgpu-1g\"\n",
        "    accelerator_count = 1\n",
        "\n",
        "# Larger setting of `max-model-len` can lead to higher requirements on\n",
        "# `gpu-memory-utilization` and GPU configuration. Larger setting of\n",
        "# `gpu-memory-utilization` increases the risk of running out of GPU memory with\n",
        "# long prompts.\n",
        "max_model_len = 4096\n",
        "gpu_memory_utilization = 0.85\n",
        "\n",
        "model, endpoint = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"mistral-serve-vllm\"),\n",
        "    model_id=prebuilt_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    max_model_len=max_model_len,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    use_openai_server=False,\n",
        "    use_chat_completions_if_openai_server=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NYN1Z49SJ-MM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07651cc-a304-4bbb-e78e-6352aa68db6a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "What is a car?\n",
            "Output:\n",
            "A car is any kind of road vehicle, typically having four wheels, that is made available for use as a personal automobile. Cars vary in size, power and features and can vary from a simple one-passenger vehicle to a luxury sedan\n"
          ]
        }
      ],
      "source": [
        "# @title Predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://github.com/vllm-project/vllm/blob/2e8e49fce3775e7704d413b2f02da6d7c99525c9/vllm/sampling_params.py#L23-L64).\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the endpoint name of\n",
        "#   the endpoint `endpoint` created in the cell above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 10  # @param {type:\"integer\"}\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "    },\n",
        "]\n",
        "response = endpoint.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# Reference the following code for using the OpenAI vLLM server.\n",
        "# import json\n",
        "# response = endpoint.raw_predict(\n",
        "#     body=json.dumps({\n",
        "#         \"model\": prebuilt_model_id,\n",
        "#         \"prompt\": \"My favourite condiment is\",\n",
        "#         \"n\": 1,\n",
        "#         \"max_tokens\": 200,\n",
        "#         \"temperature\": 1.0,\n",
        "#         \"top_p\": 1.0,\n",
        "#         \"top_k\": 10,\n",
        "#     }),\n",
        "#     headers={\"Content-Type\": \"application/json\"},\n",
        "# )\n",
        "# print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OpdxFlwikbeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f46ca54-bc04-4ea0-bb65-d1b511f227aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Undeploying Endpoint model: projects/388046178334/locations/us-central1/endpoints/7018831431954595840\n",
            "INFO:google.cloud.aiplatform.models:Undeploy Endpoint model backing LRO: projects/388046178334/locations/us-central1/endpoints/7018831431954595840/operations/648990620945219584\n",
            "INFO:google.cloud.aiplatform.models:Endpoint model undeployed. Resource name: projects/388046178334/locations/us-central1/endpoints/7018831431954595840\n",
            "INFO:google.cloud.aiplatform.base:Deleting Endpoint : projects/388046178334/locations/us-central1/endpoints/7018831431954595840\n",
            "INFO:google.cloud.aiplatform.base:Delete Endpoint  backing LRO: projects/388046178334/locations/us-central1/operations/358789919956533248\n",
            "INFO:google.cloud.aiplatform.base:Endpoint deleted. . Resource name: projects/388046178334/locations/us-central1/endpoints/7018831431954595840\n",
            "INFO:google.cloud.aiplatform.base:Deleting Model : projects/388046178334/locations/us-central1/models/1080671496034058240\n",
            "INFO:google.cloud.aiplatform.base:Delete Model  backing LRO: projects/388046178334/locations/us-central1/models/1080671496034058240/operations/7980006389374255104\n",
            "INFO:google.cloud.aiplatform.base:Model deleted. . Resource name: projects/388046178334/locations/us-central1/models/1080671496034058240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing gs://suzuki-talk-to-manual-tmp-20240506170451/...\n"
          ]
        }
      ],
      "source": [
        "# @title Clean up resources\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continouous charges that may incur.\n",
        "\n",
        "endpoint.delete(force=True)\n",
        "model.delete()\n",
        "\n",
        "delete_bucket = True  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
